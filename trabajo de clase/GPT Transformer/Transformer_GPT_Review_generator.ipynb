{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d996518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the backend to TensorFlow. The code works with\n",
    "# both `tensorflow` and `torch`. It does not work with JAX\n",
    "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
    "# (used in `causal_attention_mask()`: `tile` in JAX does\n",
    "# not support a dynamic `reps` argument.\n",
    "# You can make the code work in JAX by wrapping the\n",
    "# inside of the `causal_attention_mask` function in\n",
    "# a decorator to prevent jit compilation:\n",
    "# `with jax.ensure_compile_time_eval():`.\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1707dd",
   "metadata": {},
   "source": [
    "## Transformer block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66377ae8",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6577f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(0, maxlen, 1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c1a85",
   "metadata": {},
   "source": [
    "## Mini GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d142596",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\",\n",
    "        loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80169d96",
   "metadata": {},
   "source": [
    "## Prepare the data for word-level language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1dd76f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf_data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf_strings.lower(input_string)\n",
    "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tensorflow.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbd229",
   "metadata": {},
   "source": [
    "## Keras callback for generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0207f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b70fe",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2022ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\soyun\\anaconda3\\envs\\aprendizaje\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soyun\\anaconda3\\envs\\aprendizaje\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "this movie is an interesting movie to watch and i 've seen ! i am a fan of it . this film is so i 'm a fan of the worst , i am so that it was a bit too disappointed with the\n",
      "\n",
      "391/391 - 283s - 723ms/step - loss: 5.4720\n",
      "Epoch 2/25\n",
      "generated text:\n",
      "this movie is a great story , the movie . the plot is just the main theme . . . . . . . . . . [UNK] . [UNK] [UNK] [UNK] [UNK] . . .i have to know what 's happening on to\n",
      "\n",
      "391/391 - 248s - 635ms/step - loss: 4.7007\n",
      "Epoch 3/25\n",
      "generated text:\n",
      "this movie is so horrible . the only reason to watch it . i am not an action movie critic in a film . i don 't know how the filmmakers had to be seen , but the only one that has no [UNK]\n",
      "\n",
      "391/391 - 260s - 664ms/step - loss: 4.4528\n",
      "Epoch 4/25\n",
      "generated text:\n",
      "this movie is about a bunch of teens in a small town where kids are going to get a little girl . i guess they should go on to make a good show like the other night . . . . [UNK] [UNK] in\n",
      "\n",
      "391/391 - 286s - 731ms/step - loss: 4.2966\n",
      "Epoch 5/25\n",
      "generated text:\n",
      "this movie is not a [UNK] , but it is a very good film . its a great story that is very good , with good performances , and the story moves . the plot is very simple and a very interesting story about\n",
      "\n",
      "391/391 - 314s - 804ms/step - loss: 4.1769\n",
      "Epoch 6/25\n",
      "generated text:\n",
      "this movie is just a little bit off of the plot , and the acting is bad , but the special effects . i think the acting was good , and the script was so bad , and i would recommend this movie .\n",
      "\n",
      "391/391 - 535s - 1s/step - loss: 4.0790\n",
      "Epoch 7/25\n",
      "generated text:\n",
      "this movie is one of the best movies . . the acting is terrible , it is the worst that is a waste of time . the acting is lousy , and the writing is so horrible . it was bad . it just\n",
      "\n",
      "391/391 - 332s - 848ms/step - loss: 3.9949\n",
      "Epoch 8/25\n",
      "generated text:\n",
      "this movie is great , a good movie . it 's just a great movie . i 'm not saying it has . the graphics . [UNK] . . . . . . . and i 'm still a big fan of the first\n",
      "\n",
      "391/391 - 384s - 981ms/step - loss: 3.9222\n",
      "Epoch 9/25\n",
      "generated text:\n",
      "this movie is a great movie . i think of this movie is very well directed . the movie is very funny to the end . i can 't believe me i am a fan of the genre of the movie but i can\n",
      "\n",
      "391/391 - 334s - 853ms/step - loss: 3.8580\n",
      "Epoch 10/25\n",
      "generated text:\n",
      "this movie is a must look at the same time . a movie that 's not the [UNK] \" is a [UNK] movie . as it has an all -star cast and a pretty good cast and an interesting story about a story .\n",
      "\n",
      "391/391 - 334s - 855ms/step - loss: 3.7995\n",
      "Epoch 11/25\n",
      "generated text:\n",
      "this movie is so bad that i can 't be seen in a film . a great acting , but i can honestly say that i was very disappointed . it 's a lot of bad acting - bad effects , and bad effects\n",
      "\n",
      "391/391 - 250s - 640ms/step - loss: 3.7470\n",
      "Epoch 12/25\n",
      "generated text:\n",
      "this movie is a great movie for the first movie i saw it at the theater . it was one of the worst of all time . i 've seen it before i saw it . it 's a movie called pecker 's [UNK]\n",
      "\n",
      "391/391 - 246s - 630ms/step - loss: 3.6988\n",
      "Epoch 13/25\n",
      "generated text:\n",
      "this movie is just plain dumb . it 's not as funny as you know . if you 're actually watching a movie . it 's not to know what you 're watching , it 's not like i guess what 's going to\n",
      "\n",
      "391/391 - 245s - 626ms/step - loss: 3.6549\n",
      "Epoch 14/25\n",
      "generated text:\n",
      "this movie is the worst movie ever and is the worst movie i have ever seen ! it has a plot . it does not even have any real substance . i 've seen the movie and it 's very hard to believe it\n",
      "\n",
      "391/391 - 246s - 629ms/step - loss: 3.6144\n",
      "Epoch 15/25\n",
      "generated text:\n",
      "this movie is so bad it is , and the effects are bad . the acting is poor , the script is awful . the story is so bad that it is actually a movie . the acting is bad , the writing ,\n",
      "\n",
      "391/391 - 245s - 628ms/step - loss: 3.5771\n",
      "Epoch 16/25\n",
      "generated text:\n",
      "this movie is a lot of fun . i 'm not even sure it 's a good movie . it 's not just great . i don 't know , but it 's a bit of the story is good but not good .\n",
      "\n",
      "391/391 - 244s - 625ms/step - loss: 3.5430\n",
      "Epoch 17/25\n",
      "generated text:\n",
      "this movie is a great , very funny story of a young man living a [UNK] , and his wife . the movie moves from a [UNK] , he meets the man who falls into an accident with [UNK] . i found a single\n",
      "\n",
      "391/391 - 244s - 624ms/step - loss: 3.5105\n",
      "Epoch 18/25\n",
      "generated text:\n",
      "this movie is a good movie with bad acting . it is the worst . it is the worst movie i have ever seen . i can 't believe me ! ! it 's the only reason i can 't believe i saw this\n",
      "\n",
      "391/391 - 245s - 626ms/step - loss: 3.4805\n",
      "Epoch 19/25\n",
      "generated text:\n",
      "this movie is really a very interesting film , very little narrative storyline . a film with lots of action scenes which are not very good . the music is so bad , and that a very good movie .    \n",
      "\n",
      "391/391 - 246s - 628ms/step - loss: 3.4528\n",
      "Epoch 20/25\n",
      "generated text:\n",
      "this movie is a good start with a good story , the acting is weak and the plot and the ending is so predictable and you don 't get to the ending . i was very surprised by the ending , the ending was\n",
      "\n",
      "391/391 - 245s - 626ms/step - loss: 3.4264\n",
      "Epoch 21/25\n",
      "generated text:\n",
      "this movie is one of the worst films i have ever seen . the only one thing is that i 've seen since it is the worst of all times . there is a lot with [UNK] , i have been a big fan\n",
      "\n",
      "391/391 - 244s - 623ms/step - loss: 3.4018\n",
      "Epoch 22/25\n",
      "generated text:\n",
      "this movie is not just a good movie . it was a movie that relies too much too much on the movie cliches but not enough , the main characters and the movie is very funny . it 's not the movie . it\n",
      "\n",
      "391/391 - 243s - 623ms/step - loss: 3.3786\n",
      "Epoch 23/25\n",
      "generated text:\n",
      "this movie is a lot of fun . the plot and the movie is a little cheesy and cheesy , but the acting is pretty cheesy . but it is just a waste of time . it does not matter as bad as many\n",
      "\n",
      "391/391 - 244s - 623ms/step - loss: 3.3573\n",
      "Epoch 24/25\n",
      "generated text:\n",
      "this movie is a perfect example of how it was made . there are many good things about a movie that should be taken to by a group of friends . i think this movie had no idea what it could have been a\n",
      "\n",
      "391/391 - 244s - 623ms/step - loss: 3.3368\n",
      "Epoch 25/25\n",
      "generated text:\n",
      "this movie is just plain fun . it has everything going to happen in the world , but it 's a great cast that should have been a big budget . it has a nice storyline with a great acting and the acting is\n",
      "\n",
      "391/391 - 244s - 625ms/step - loss: 3.3171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f91e7e4dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
