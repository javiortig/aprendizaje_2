{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67941fe7",
   "metadata": {},
   "source": [
    "# Modelo de transformer para un mini GPT\n",
    "Realizamos cambios al codigo existente en un gpt [cuya referencia se encuentra en el siguiente link](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "Los comentarios se mantendrán en ingles para mantener cohesion con el resto del codigo base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aeae8d",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d996518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1707dd",
   "metadata": {},
   "source": [
    "## Transformer block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66377ae8",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6577f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c1a85",
   "metadata": {},
   "source": [
    "## Modelo Mini GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d142596",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000  # Tamaño del vocabulario de 30k palabras\n",
    "maxlen = 70  # Tamaño máximo\n",
    "embed_dim = 256  # Tamaño del embedding para cada token\n",
    "feed_forward_dim = 256  # Tamaño de hidden layer\n",
    "num_heads = 2\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\",\n",
    "        loss=[loss_fn, None],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80169d96",
   "metadata": {},
   "source": [
    "## Prepara los datos para la ingestión del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe591d0",
   "metadata": {},
   "source": [
    "Creamos un dataset a partir de los titulares. Puesto que el dataset es enano 1k de lineas, lo hemos duplicado, además de ajustar ciertos parámetros:\n",
    "* Un vocabulario más amplio\n",
    "* `maxlen` más grande para parecerse más a los títulos del dataset\n",
    "* Un buffer más grande en el shuffle, puesto que el dataset es pequeño y nos lo podemos permitir.\n",
    "* `text_ds.repeat()` para asegurarnos que durante el entrenamiento el modelo no se quede sin datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "file_path = './clean_titulares.txt'\n",
    "\n",
    "# Generamos el dataset\n",
    "text_ds = tf.data.TextLineDataset(file_path)\n",
    "text_ds = text_ds.shuffle(buffer_size=1024)  # Nos podemos permitir un buffer grande\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "# Adaptamos el dataset\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=maxlen + 1\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # Recuperar las palabras a raiz de los tokens\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "# Repite el proceso tantas veces como haga falta para no quedarnos sin datos durante el entrenamiento\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n",
    "text_ds = text_ds.repeat() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbd229",
   "metadata": {},
   "source": [
    "## Callback para generar el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1999d68",
   "metadata": {},
   "source": [
    "Aquí hemos modificado los tokens iniciales, puesto que lo que buscamos es generar títulos, no reviews de películas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"Texto generado:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenizamos un prompt inicial\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"el\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b70fe",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596806e",
   "metadata": {},
   "source": [
    "Hemos ajustado los steps por época para garantizar el uso del dataset completo de forma uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2022ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "Texto generado:\n",
      "el la el        de     la        del  y  para con  la para  en     para   \n",
      "\n",
      "16/16 - 26s - 2s/step - loss: 7.2123\n",
      "Epoch 2/25\n",
      "Texto generado:\n",
      "el que la era                                      \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 1.5435\n",
      "Epoch 3/25\n",
      "Texto generado:\n",
      "el y el futuro y la moda en un mundo                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.5459\n",
      "Epoch 4/25\n",
      "Texto generado:\n",
      "el futuro de enfermedades                                      \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.4288\n",
      "Epoch 5/25\n",
      "Texto generado:\n",
      "el cine para un planeta en la ia                                  \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.3492\n",
      "Epoch 6/25\n",
      "Texto generado:\n",
      "el impacto de la salud mental en la pantalla                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.3017\n",
      "Epoch 7/25\n",
      "Texto generado:\n",
      "el impacto de la gratitud y desafios                                   \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.2666\n",
      "Epoch 8/25\n",
      "Texto generado:\n",
      "el arte de enfermedades con estilo y el bienestar emocional                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.2387\n",
      "Epoch 9/25\n",
      "Texto generado:\n",
      "el cine de la educacion aprendizaje en la industria de los estudiantes                              \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.2224\n",
      "Epoch 10/25\n",
      "Texto generado:\n",
      "el impacto de la inteligencia emocional en la ia                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.2028\n",
      "Epoch 11/25\n",
      "Texto generado:\n",
      "el futuro de la inteligencia artificial en la vida cotidiana                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1870\n",
      "Epoch 12/25\n",
      "Texto generado:\n",
      "el impacto de la contaminacion y la salud mental                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1755\n",
      "Epoch 13/25\n",
      "Texto generado:\n",
      "el auge de la educacion impulsado por la moda                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1635\n",
      "Epoch 14/25\n",
      "Texto generado:\n",
      "el cine documental como herramienta de cambio social                                  \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1518\n",
      "Epoch 15/25\n",
      "Texto generado:\n",
      "el impacto de la colonizacion cinematografica en las relaciones personales                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1417\n",
      "Epoch 16/25\n",
      "Texto generado:\n",
      "el impacto de la educacion impulsado por la salud mental                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1302\n",
      "Epoch 17/25\n",
      "Texto generado:\n",
      "el papel de la mentalidad positiva en la vida cotidiana                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1199\n",
      "Epoch 18/25\n",
      "Texto generado:\n",
      "el bienestar emocional en tiempos de estres y ansiedad                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1100\n",
      "Epoch 19/25\n",
      "Texto generado:\n",
      "el arte contemporaneo deslumbra en la galeria de recursos marinos                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.1027\n",
      "Epoch 20/25\n",
      "Texto generado:\n",
      "el papel de la tecnologia de la interconexion                                  \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.0931\n",
      "Epoch 21/25\n",
      "Texto generado:\n",
      "el cine documental como motor de cambio social                                  \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.0874\n",
      "Epoch 22/25\n",
      "Texto generado:\n",
      "el impacto de las historias reales en la pantalla                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.0815\n",
      "Epoch 23/25\n",
      "Texto generado:\n",
      "el futuro de la colonizacion en linea                                   \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.0755\n",
      "Epoch 24/25\n",
      "Texto generado:\n",
      "el poder de las historias reales en la pantalla                                 \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.0725\n",
      "Epoch 25/25\n",
      "Texto generado:\n",
      "el impacto de la pandemia en la atencion al clima                                \n",
      "\n",
      "16/16 - 23s - 1s/step - loss: 0.0698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x225fe2db850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcula el los steps necesarios por época\n",
    "total_samples = sum(1 for _ in open(file_path, 'r'))\n",
    "steps_per_epoch = total_samples // batch_size\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(text_ds, steps_per_epoch=steps_per_epoch, verbose=2, epochs=25, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506ecf7",
   "metadata": {},
   "source": [
    "Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "468162d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './GPT_Transformer.keras'\n",
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b201d77",
   "metadata": {},
   "source": [
    "## Generador titulos a partir de nuestro modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d87bf",
   "metadata": {},
   "source": [
    "Para ello, partimos de una lista de prompts iniciales. Es importante usar vocabulario que se encuentre en el dataset a la hora de elegir los prompts iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbcd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacia un mundo mas verde                                      \n",
      "la historia de la realidad aumentada integracion en la inclusion                                \n",
      "diagnosticos y tecnicas con la medicina de precision                                    \n",
      "cuando el auge de la medicina de los coches electricos                                \n",
      "el futuro a traves de la forma en la justicia social                                 \n",
      "las ciudades de precision culinarias en el mundo                                  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "start_prompt = \"el\"\n",
    "\n",
    "def generate_text(model, start_prompt, num_tokens_to_generate, vocab, word_to_index, top_k=10):\n",
    "    start_tokens = [word_to_index.get(word, word_to_index.get(\"[UNK]\", 1)) for word in start_prompt.split()]\n",
    "    input_tokens = start_tokens.copy()\n",
    "\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        # prepara el input (1, maxlen)\n",
    "        trimmed = input_tokens[-maxlen:]\n",
    "        pad_len = maxlen - len(trimmed)\n",
    "        x = trimmed + [0] * pad_len\n",
    "        input_array = np.array([x], dtype=np.int32)\n",
    "\n",
    "        # Obtenemos las predicciones\n",
    "        logits = model.predict(input_array, verbose=0)[0]\n",
    "        last_valid_index = len(trimmed) - 1\n",
    "        predictions = logits[0, last_valid_index, :]  # Predice el siguiente token\n",
    "\n",
    "        next_token = sample_from(predictions, top_k)\n",
    "        input_tokens.append(next_token)\n",
    "\n",
    "    # Detokenizar\n",
    "    generated_words = []\n",
    "    for token in input_tokens:\n",
    "        if 0 <= token < len(vocab):\n",
    "            generated_words.append(vocab[token])\n",
    "        else:\n",
    "            generated_words.append(\"[UNK]\")\n",
    "\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "\n",
    "def sample_from(logits, top_k=10):\n",
    "    \"\"\"\n",
    "    Sample an index from logits array with probability proportional to the softmax of logits.\n",
    "    \n",
    "    Args:\n",
    "    logits (np.array): Array of logits from model predictions.\n",
    "    top_k (int): Top k logits to consider for sampling.\n",
    "    \n",
    "    Returns:\n",
    "    int: Sampled index corresponding to predicted word.\n",
    "    \"\"\"\n",
    "    logits, indices = tf.math.top_k(logits, k=top_k)\n",
    "    probabilities = tf.nn.softmax(logits).numpy()\n",
    "    sampled_index = np.random.choice(indices.numpy(), p=probabilities)\n",
    "    return sampled_index\n",
    "\n",
    "\n",
    "num_tokens_generated = 45\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Generamos un texto a raiz de una lista de prompts iniciales\n",
    "initial_prompts = [\n",
    "    \"hacia un mundo\",\n",
    "    \"la historia\",\n",
    "    \"diagnosticos y tecnicas con\",\n",
    "    \"cuando el\",\n",
    "    \"el futuro a traves\",\n",
    "    \"las ciudades\"\n",
    "    ]\n",
    "\n",
    "for prompt in initial_prompts:\n",
    "    generated_text = generate_text(model, prompt, 40, vocab, word_to_index)\n",
    "    print(f\"{generated_text}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
